{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713241f",
   "metadata": {},
   "source": [
    "# Building a Text-to-SQL ReAct Agent\n",
    "\n",
    "We'll build a **ReAct agent** using [DSPy](https://dspy.ai/) that translates natural language questions into SQL queries against a quick-commerce DuckDB database (order data from Pune, India).\n",
    "\n",
    "**What we'll cover:**\n",
    "1. **Build** — Connect to DuckDB, define a DSPy ReAct agent\n",
    "2. **Evaluate** — Score the agent using metrics\n",
    "4. **Optimize** — Use DSPy optimizers to automatically tune the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4399670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import dspy\n",
    "import duckdb\n",
    "from compare import display_comparison\n",
    "from dotenv import load_dotenv\n",
    "from eval_utils import (\n",
    "    get_last_analytical_sql,\n",
    "    run_eval,\n",
    "    run_single_eval,\n",
    "    save_eval,\n",
    ")\n",
    "from trajectory import save_trajectory\n",
    "\n",
    "from data.eval_dataset import DATASET\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c99055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB: connect to the database file\n",
    "db_conn = duckdb.connect(\"../data/qc_pune.duckdb\", read_only=True)\n",
    "\n",
    "\n",
    "def execute_sql(sql: str, limit: int = 50) -> str:\n",
    "    \"\"\"Execute a SQL query against the DuckDB database.\n",
    "\n",
    "    Use SHOW TABLES to discover tables, DESCRIBE table_name for schema.\n",
    "    Use LIMIT on exploratory queries. Write valid DuckDB SQL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = db_conn.execute(sql).fetchall()\n",
    "        if not result:\n",
    "            return \"(No rows returned)\"\n",
    "\n",
    "        columns = [desc[0] for desc in db_conn.description]\n",
    "        header = \" | \".join(columns)\n",
    "        rows = \"\\n\".join(\" | \".join(str(v) for v in row) for row in result[:limit])\n",
    "        total = len(result)\n",
    "        note = f\"\\n... ({total} total rows)\" if total > limit else \"\"\n",
    "        return f\"{header}\\n{rows}{note}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"SQL Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4bb54a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "consumers\n",
      "itemized_orders\n",
      "product_catalogue\n",
      "stores\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"SHOW TABLES;\"\n",
    "print(execute_sql(sql_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff0ead",
   "metadata": {},
   "source": [
    "```text\n",
    "┌──────────────────────────────┐                ┌──────────────────────────────────────┐\n",
    "│          consumers           │                │           itemized_orders            │\n",
    "├──────────────────────────────┤                ├──────────────────────────────────────┤\n",
    "│ VARCHAR  consumer_id     PK  │◀┐              │ VARCHAR   order_id              PK   │\n",
    "│ VARCHAR  address_id      PK2 │◀┼┐             │ INTEGER   order_line_id         PK2  │\n",
    "│ VARCHAR  gender              │ └│─────────────│ VARCHAR   consumer_id           FK   │\n",
    "│ INTEGER  age                 │  └─────────────│ VARCHAR   address_id            FK   │\n",
    "│ DOUBLE   latitude            │            ┌───│ VARCHAR   store_id              FK   │\n",
    "│ DOUBLE   longitude           │          ┌─│───│ VARCHAR   product_id            FK   │\n",
    "│ VARCHAR  address             │          │ │   │ INTEGER   quantity                   │\n",
    "│ VARCHAR  pincode             │          │ │   │ DOUBLE    mrp                        │\n",
    "│ VARCHAR  nearest_store_id FK │──┐       │ │   │ DOUBLE    discount                   │\n",
    "└──────────────────────────────┘  │       │ │   │ DOUBLE    item_total                 │\n",
    "                                  │       │ │   │ TIMESTAMP order_timestamp            │\n",
    "┌──────────────────────────────┐  │       │ │   │ VARCHAR   order_status               │\n",
    "│      product_catalogue       │  │       │ │   │ VARCHAR   cancel_reason              │\n",
    "├──────────────────────────────┤  │       │ │   │ INTEGER   committed_delivery_secs    │\n",
    "│ VARCHAR  product_id      PK  │◀─┼───────┘ │   │ INTEGER   actual_delivery_secs       │\n",
    "│ VARCHAR  product_name        │  │         │   │ DOUBLE    distance_km                │\n",
    "│ VARCHAR  brand_name          │  │         │   └──────────────────────────────────────┘\n",
    "│ VARCHAR  brand_id            │  │         │\n",
    "│ VARCHAR  category            │  │         │   ┌──────────────────────────────────────┐\n",
    "│ VARCHAR  sub_category        │  │         │   │                stores                │\n",
    "│ VARCHAR  size                │  │         │   ├──────────────────────────────────────┤\n",
    "│ DOUBLE   mrp                 │  └─────────└──▶│ VARCHAR  store_id              PK    │\n",
    "│ VARCHAR  category_code       │                │ VARCHAR  store_name                  │\n",
    "│ VARCHAR  sub_category_code   │                │ DOUBLE   latitude                    │\n",
    "└──────────────────────────────┘                │ DOUBLE   longitude                   │\n",
    "                                                │ DOUBLE   radius_km                   │\n",
    "                                                │ VARCHAR  pincode                     │\n",
    "                                                │ VARCHAR  area                        │\n",
    "                                                │ VARCHAR  serviceable_area            │\n",
    "                                                └──────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "sql_query = \"\"\"\"\"\"\n",
    "print(execute_sql(sql_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy LM via OpenRouter\n",
    "lm = dspy.LM(\n",
    "    \"openrouter/google/gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    max_tokens=100000,\n",
    "    cache=False,\n",
    ")\n",
    "dspy.configure(lm=lm, adapter=dspy.ChatAdapter(), track_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e44d25",
   "metadata": {},
   "source": [
    "## Text-to-SQL ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2c54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct Agent for Data Analysis\n",
    "class AnalystSignature(dspy.Signature):\n",
    "    \"\"\"You are a data analyst working with a DuckDB database.\n",
    "\n",
    "    Given a user's question, explore the database schema, write and execute\n",
    "    SQL queries, and provide a clear, accurate answer grounded in the data.\n",
    "\n",
    "    Start by discovering available tables (SHOW TABLES) and their schemas\n",
    "    (DESCRIBE table_name) before writing analytical queries.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"A natural language question about the data\")\n",
    "    answer: str = dspy.OutputField(desc=\"A concise, data-backed answer to the question\")\n",
    "\n",
    "\n",
    "agent_v1 = dspy.ReAct(AnalystSignature, tools=[execute_sql], max_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3eeb041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 product categories that are most often ordered on weekends are:\n",
      "1. **Personal Care**: 193,562 orders\n",
      "2. **Instant & Ready to Eat**: 192,188 orders\n",
      "3. **Beverages**: 192,144 orders\n"
     ]
    }
   ],
   "source": [
    "weekend_orders = agent_v1(\n",
    "    question=\"Which top 3 product categories that are often ordered on weekends? Give total order count for each category as well.\"\n",
    ")\n",
    "print(weekend_orders.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e73535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trajectory and LM usage to a markdown file\n",
    "md_path = save_trajectory(\n",
    "    weekend_orders.trajectory,\n",
    "    weekend_orders.get_lm_usage(),\n",
    "    name=\"weekend_orders_agent_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwrnissnssh",
   "metadata": {},
   "source": [
    "## Marker Agent\n",
    "\n",
    "Once the agent has answered, what this agent does is:\n",
    "- Identify what the SQL query actually does — a simple summary so you can verify it understood your question\n",
    "- Does it actually answer what you asked? — sometimes the agent fetches data that looks related but doesn't quite answer the question\n",
    "- Checks what might be missing — e.g. it filtered out cancelled orders when you wanted all orders, or it counted line items instead of unique orders\n",
    "- How confident we are — a gut-check on whether you should trust the answer or double-check it yourself\n",
    "\n",
    "We need this because the agent can return confident-sounding answers that are subtly wrong — wrong grouping, missing filters, counting rows instead of orders. Without an check, those errors silently reach the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fyc70s2sggs",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkerSignature(dspy.Signature):\n",
    "    \"\"\"Analyze the agent's SQL query and answer for a given user question.\n",
    "\n",
    "    Explain what the SQL does in plain English, assess whether the output\n",
    "    answers the user's question, identify any gaps, and rate your confidence.\n",
    "    \"\"\"\n",
    "\n",
    "    user_query: str = dspy.InputField(desc=\"The original natural language question\")\n",
    "    final_generated_sql: str = dspy.InputField(\n",
    "        desc=\"The final analytical SQL the agent executed\"\n",
    "    )\n",
    "    answer: str = dspy.InputField(desc=\"The agent's textual answer to the question\")\n",
    "    humane_version_of_sql: str = dspy.OutputField(\n",
    "        desc=\"Plain-English explanation of what the SQL does without mentioning techincal details so that it can be understood by non-technical person. Give list in markdown format if there are multiple steps.\"\n",
    "    )\n",
    "    does_op_answer_user_query: str = dspy.OutputField(\n",
    "        desc=\"Yes/No with brief justification — does the output answer the question?\"\n",
    "    )\n",
    "    gaps_in_answer: str = dspy.OutputField(\n",
    "        desc=\"What is missing, wrong, or could be improved in the answer\"\n",
    "    )\n",
    "    confidence: Literal[\"low\", \"medium\", \"high\"] = dspy.OutputField(\n",
    "        desc=\"Confidence in the answer quality\"\n",
    "    )\n",
    "\n",
    "\n",
    "marker = dspy.ChainOfThought(MarkerSignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "p1y6liwlqan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Explanation:\n",
      "To find the top 3 product categories ordered on weekends, the system did the following:\n",
      "1. Looked at all customer orders.\n",
      "2. Filtered these orders to only include those placed on Saturdays or Sundays.\n",
      "3. For each of these weekend orders, identified the category of the product purchased.\n",
      "4. Counted how many unique orders each product category received on weekends.\n",
      "5. Ranked the product categories from most to least ordered on weekends.\n",
      "6. Picked out the top 3 categories from this ranked list.\n",
      "\n",
      "Answers the question?:\n",
      "Yes, the output directly identifies the top 3 product categories most often ordered on weekends, as requested by the user.\n",
      "\n",
      "Gaps:\n",
      "There are no significant gaps in the answer. It directly addresses the user's query clearly and concisely.\n",
      "\n",
      "Confidence:\n",
      "high\n"
     ]
    }
   ],
   "source": [
    "# Run marker on the weekend_orders result\n",
    "agent_sql = get_last_analytical_sql(weekend_orders.trajectory)\n",
    "\n",
    "marked = marker(\n",
    "    user_query=\"Which top 3 product categories that are often ordered on weekends?\",\n",
    "    final_generated_sql=agent_sql or \"(no SQL)\",\n",
    "    answer=weekend_orders.answer,\n",
    ")\n",
    "\n",
    "print(f\"SQL Explanation:\\n{marked.humane_version_of_sql}\\n\")\n",
    "print(f\"Answers the question?:\\n{marked.does_op_answer_user_query}\\n\")\n",
    "print(f\"Gaps:\\n{marked.gaps_in_answer}\\n\")\n",
    "print(f\"Confidence:\\n{marked.confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "query_result = agent_v1(question=\"\")\n",
    "print(query_result.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e3572",
   "metadata": {},
   "source": [
    "## Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9498ab0",
   "metadata": {},
   "source": [
    "### What are Evals?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dba3b0",
   "metadata": {},
   "source": [
    "### Metric 1: Tool Efficiency\n",
    "\n",
    "**Why we need this?**\n",
    "- Every tool call is like an LLM call — it costs money and adds latency. An agent that takes 8 calls to answer \"how many products are there?\" works, but it's 4x slower and 4x more expensive than one that does it in 2 tool calls.\n",
    "- This metric tells you whether the agent is exploring the database efficiently or burning tokens on unnecessary SHOW TABLES / DESCRIBE / trial-and-error queries before getting to the actual answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b25174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_efficiency(trajectory: dict) -> float:\n",
    "    \"\"\"Score based on number of tool calls. Fewer is better.\"\"\"\n",
    "    i = 0\n",
    "    while f\"observation_{i}\" in trajectory:\n",
    "        i += 1\n",
    "    if i <= 4:\n",
    "        return 1.0\n",
    "    if i <= 7:\n",
    "        return 0.5\n",
    "    return 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ecd5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool efficiency: 0.50\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tool efficiency: {tool_efficiency(weekend_orders.trajectory):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j7klg8a0gqp",
   "metadata": {},
   "source": [
    "### Metric 2: SQL Validity\n",
    "\n",
    "**Why we need this?**\n",
    "\n",
    "Measures what fraction of the agent's SQL queries actually ran without errors. If the agent writes SELECT * FROM ordes (typo), that's a failed call — wasted money, added latency, and it means the agent is guessing at table/column names instead of knowing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5zpo8qr65hr",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_validity(trajectory: dict) -> float:\n",
    "    \"\"\"Fraction of SQL tool calls that did NOT return an error.\"\"\"\n",
    "    errors = total = 0\n",
    "    i = 0\n",
    "    while f\"observation_{i}\" in trajectory:\n",
    "        if trajectory.get(f\"tool_name_{i}\") == \"execute_sql\":\n",
    "            total += 1\n",
    "            if trajectory[f\"observation_{i}\"].startswith(\"SQL Error:\"):\n",
    "                errors += 1\n",
    "        i += 1\n",
    "    return (total - errors) / total if total else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blwmkj8wax5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL validity: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"SQL validity: {sql_validity(weekend_orders.trajectory):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jzn9xyhoqnd",
   "metadata": {},
   "source": [
    "### Metric 3: Error Recovery\n",
    "\n",
    "**Why we need this?**\n",
    "\n",
    "An agent that makes a mistake isn't necessarily bad, as long as it fixes itself. This measures whether the agent recovered after hitting an error. A low score here means the agent is fragile — one wrong query and it falls apart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eg4ekj228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_recovery(trajectory: dict, answer: str) -> float:\n",
    "    \"\"\"Did the agent self-correct after SQL errors?\"\"\"\n",
    "    errors = 0\n",
    "    i = 0\n",
    "    while f\"observation_{i}\" in trajectory:\n",
    "        if trajectory.get(f\"tool_name_{i}\") == \"execute_sql\":\n",
    "            if trajectory[f\"observation_{i}\"].startswith(\"SQL Error:\"):\n",
    "                errors += 1\n",
    "        i += 1\n",
    "    if errors == 0:\n",
    "        return 1.0\n",
    "    if answer and answer != \"(No response)\":\n",
    "        return 0.75\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "z67lfcplqh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error recovery: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Error recovery: {error_recovery(weekend_orders.trajectory, weekend_orders.answer):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fwvg19lz7m",
   "metadata": {},
   "source": [
    "### Metric 4: Answer Quality\n",
    "\n",
    "**Why we need this?**\n",
    "\n",
    "- The other three metrics tell you how the agent worked — was it fast, clean, resilient. But none of them tell you if the answer is actually correct.\n",
    "- An agent can score 1.0 on efficiency, 1.0 on SQL validity, 1.0 on recovery — and still return completely wrong data. It wrote valid SQL, it did it in 2 calls, it never errored... but it counted line items instead of orders, so the number is 3x too high.\n",
    "- Answer Quality is the only metric that catches that. It's the difference between \"the agent ran smoothly\" and \"the agent gave you correct output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tcvmldauu8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerQualitySignature(dspy.Signature):\n",
    "    \"\"\"Rate how well the agent answered a data analysis question.\n",
    "\n",
    "    Evaluate both the SQL approach and the output data holistically.\n",
    "\n",
    "    For the SQL query, ignore superficial differences such as:\n",
    "    - Aliases and formatting\n",
    "    - JOIN syntax variations\n",
    "    - Column order\n",
    "    - Equivalent date functions\n",
    "    - Additional ORDER BY/LIMIT clauses\n",
    "\n",
    "    For the output data, ignore superficial differences such as:\n",
    "    - Different column names for the same data (e.g. \"store\" vs \"store_name\")\n",
    "    - Different row ordering\n",
    "    - Minor rounding differences (e.g. 14.7 vs 14.71)\n",
    "    - Extra columns that don't change the core answer\n",
    "    - Different date/time formatting (e.g. \"2025-11\" vs \"2025-11-01\")\n",
    "\n",
    "    Consider the answer incorrect only if the core logic or data differs:\n",
    "    - Wrong GROUP BY, aggregations, filters, or JOINs\n",
    "    - Significantly different row counts or wrong values\n",
    "    - Missing key columns that the question asked for\n",
    "    \"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The natural language question\")\n",
    "    reference_sql: str = dspy.InputField(desc=\"The ground truth SQL query\")\n",
    "    agent_sql: str = dspy.InputField(desc=\"The agent-generated SQL query\")\n",
    "    expected_csv: str = dspy.InputField(desc=\"The expected output data (CSV)\")\n",
    "    agent_csv: str = dspy.InputField(desc=\"The agent-generated output data (CSV)\")\n",
    "    score: float = dspy.OutputField(desc=\"Score between 0 and 1\")\n",
    "\n",
    "\n",
    "judge = dspy.ChainOfThought(AnswerQualitySignature)\n",
    "\n",
    "\n",
    "def answer_quality(agent_sql, reference_sql, expected_csv, question, conn):\n",
    "    \"\"\"Execute agent SQL and use an LLM judge to score the result.\"\"\"\n",
    "    if not agent_sql:\n",
    "        return {\"score\": 0.0, \"reasoning\": \"No analytical SQL found\", \"agent_csv\": None}\n",
    "\n",
    "    try:\n",
    "        result_df = conn.execute(agent_sql).fetchdf()\n",
    "        agent_csv_str = result_df.to_csv(index=False)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"reasoning\": f\"SQL execution error: {e}\",\n",
    "            \"agent_csv\": None,\n",
    "        }\n",
    "\n",
    "    agent_csv_full = agent_csv_str\n",
    "\n",
    "    # Truncate for prompt size\n",
    "    agent_lines = agent_csv_str.strip().split(\"\\n\")\n",
    "    if len(agent_lines) > 51:\n",
    "        agent_csv_str = (\n",
    "            \"\\n\".join(agent_lines[:51]) + f\"\\n... ({len(agent_lines) - 1} total rows)\"\n",
    "        )\n",
    "\n",
    "    expected_lines = expected_csv.strip().split(\"\\n\")\n",
    "    if len(expected_lines) > 51:\n",
    "        expected_csv = (\n",
    "            \"\\n\".join(expected_lines[:51])\n",
    "            + f\"\\n... ({len(expected_lines) - 1} total rows)\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        result = judge(\n",
    "            question=question,\n",
    "            reference_sql=reference_sql,\n",
    "            agent_sql=agent_sql,\n",
    "            expected_csv=expected_csv,\n",
    "            agent_csv=agent_csv_str,\n",
    "        )\n",
    "        return {\n",
    "            \"score\": float(result.score),\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"agent_csv\": agent_csv_full,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"reasoning\": f\"Judge error: {e}\",\n",
    "            \"agent_csv\": agent_csv_full,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "t7vgo66ojx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.2\n",
      "Reasoning: The agent failed to include the `order_status = 'delivered'` filter. As a result, the `total_orders` counts are significantly different from the `weekend_order_count` in the expected output. While the top 3 categories are the same, the values are incorrect due to the missing filter, leading to a low score.\n"
     ]
    }
   ],
   "source": [
    "# Example: score the agent's SQL + output against reference\n",
    "agent_sql = get_last_analytical_sql(weekend_orders.trajectory)\n",
    "reference_sql = \"\"\"\n",
    "SELECT pc.category, COUNT(DISTINCT io.order_id) AS weekend_order_count\n",
    "FROM itemized_orders io\n",
    "JOIN product_catalogue pc ON io.product_id = pc.product_id\n",
    "WHERE DAYOFWEEK(io.order_timestamp) IN (1, 7)\n",
    "  AND io.order_status = 'delivered'\n",
    "GROUP BY pc.category\n",
    "ORDER BY weekend_order_count DESC\n",
    "LIMIT 3;\n",
    "\"\"\"\n",
    "expected_csv = \"category,weekend_order_count\\nPersonal Care,76915\\nInstant & Ready to Eat,75652\\nBeverages,75555\"\n",
    "\n",
    "result = answer_quality(\n",
    "    agent_sql=agent_sql,\n",
    "    reference_sql=reference_sql,\n",
    "    expected_csv=expected_csv,\n",
    "    question=\"Which top 3 product categories are often ordered on weekends?\",\n",
    "    conn=db_conn,\n",
    ")\n",
    "print(f\"Score: {result['score']}\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f968e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eval on 16 examples with agent_v1\n",
      "\n",
      "#   ID                                  Difficulty   Efficiency   SQLValid   Recovery   Answer Quality\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1   1_product_count                     easy         1.00         1.00       1.00       1.00       (28.2s)\n",
      "2   2_unique_categories                 easy         1.00         1.00       1.00       1.00       (17.9s)\n",
      "3   3_store_count                       easy         1.00         1.00       1.00       1.00       (5.4s)\n",
      "4   4_avg_age_by_gender                 easy         1.00         1.00       1.00       0.90       (8.1s)\n",
      "5   5_orders_by_status                  easy         0.50         1.00       1.00       1.00       (18.7s)\n",
      "6   6_top_store_delivered               easy         0.50         1.00       1.00       0.00       (16.6s)\n",
      "7   7_store_performance_scorecard       medium       0.50         1.00       1.00       0.70       (40.3s)\n",
      "8   8_demand_hour_dayofweek             medium       0.50         1.00       1.00       0.50       (25.1s)\n",
      "9   9_user_retention                    hard         0.50         0.80       0.75       0.70       (44.9s)\n",
      "10  10_basket_affinity                  hard         0.50         1.00       1.00       0.80       (31.9s)\n",
      "11  11_high_value_churned               hard         0.25         0.86       0.75       0.30       (59.2s)\n",
      "12  12_cancellation_rca                 hard         0.50         1.00       1.00       0.70       (65.6s)\n",
      "13  13_4week_moving_avg                 hard         0.50         1.00       1.00       0.75       (20.8s)\n",
      "14  14_delivery_distance_sla_breach     hard         1.00         1.00       1.00       0.50       (25.5s)\n",
      "15  15_impossible_salary                impossible   1.00         1.00       1.00       0.00       (9.1s)\n",
      "16  16_impossible_weather               impossible   1.00         1.00       1.00       1.00       (12.0s)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "AVG                                                  0.70         0.98       0.97       0.68       (429s)\n"
     ]
    }
   ],
   "source": [
    "eval_results = run_eval(agent_v1, DATASET, db_conn, agent_name=\"agent_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c2k5d9dm",
   "metadata": {},
   "source": [
    "## Add Your Own Eval\n",
    "\n",
    "Use `save_eval` to create a new eval example from a question + SQL query. It runs the SQL, saves the CSV and SQL files, and returns a dataset entry you can evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6krjagiukqw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 17_top_brands_snacks:\n",
      "  SQL → eval_answer_sqls/17_top_brands_snacks.sql\n",
      "  CSV → eval_answer_csvs/17_top_brands_snacks.csv (3 rows)\n"
     ]
    }
   ],
   "source": [
    "# Example: save a new eval\n",
    "new_example = save_eval(\n",
    "    eval_id=\"17_top_brands_snacks\",\n",
    "    question=\"What are the top 3 brands by revenue in the Snacks & Munchies category?\",\n",
    "    sql=\"\"\"\n",
    "SELECT pc.brand_name, SUM(io.item_total) AS total_revenue\n",
    "FROM itemized_orders io\n",
    "JOIN product_catalogue pc ON io.product_id = pc.product_id\n",
    "WHERE pc.category = 'Snacks & Munchies'\n",
    "  AND io.order_status = 'delivered'\n",
    "GROUP BY pc.brand_name\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 3;\n",
    "\"\"\",\n",
    "    conn=db_conn,\n",
    "    difficulty=\"medium\",\n",
    "    expected_tables=[\"itemized_orders\", \"product_catalogue\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9bbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "sql_query = \"\"\"\"\"\"\n",
    "print(execute_sql(sql_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "new_example = save_eval(\n",
    "    eval_id=\"\",  # eg \"17\" or \"top_brands_snack\" or \"17_top_brands_snacks\"\n",
    "    question=\"\",  # eg \"What are the top 3 brands by revenue in the Snacks & Munchies category?\"\n",
    "    sql=\"\"\"\"\"\",  # eg the SQL query the agent generated for that question\n",
    "    conn=db_conn,\n",
    "    difficulty=\"\",  # easy, medium or hard\n",
    "    expected_tables=[],  # list of tables that should be used in the SQL answer, e.g. [\"itemized_orders\", \"product_catalogue\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "conl2n3ewtt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".compare-table { font-size: 12px; border-collapse: collapse; width: 100%;\n",
       "                 color: #000; background: #fff; }\n",
       ".compare-table th { background: #f0f0f0; color: #000; padding: 6px 8px; text-align: left;\n",
       "                     border-bottom: 2px solid #ddd; white-space: nowrap; }\n",
       ".compare-table td { padding: 4px 8px; border-bottom: 1px solid #eee; color: #000; }\n",
       "</style>\n",
       "\n",
       "<div style=\"background:#fff;border:1px solid #ddd;border-radius:8px;padding:20px;margin-bottom:24px;font-family:system-ui,sans-serif;color:#000;\">\n",
       "<div style=\"font-size:15px;font-weight:600;margin-bottom:12px;color:#000;\">What are the top 3 brands by revenue in the Snacks &amp; Munchies category?</div>\n",
       "<div style=\"margin-bottom:16px;\"><div style=\"display:block;margin-right:16px;\"><span style=\"background:#eab308;color:#fff;padding:4px 10px;border-radius:4px;font-weight:600;font-size:14px;\">Answer Quality: 0.50</span><div style=\"color:#000;font-size:12px;margin-top:4px;\">The agent-generated SQL query is missing a crucial `WHERE` clause condition: `io.order_status = &#x27;delivered&#x27;`. This omission leads to a different calculation of `total_revenue`, as it includes items from orders that are not yet delivered. Consequently, the `agent_csv` has different (higher) revenue figures compared to the `expected_csv`, making the agent&#x27;s answer incorrect.\n",
       "\n",
       "Despite the SQL having a correctness issue, the structure of the query (joins, group by, order by, limit) is correct for the question, just one filter was missed. The output format is also correct. Therefore, it warrants a partial score.</div></div></div>\n",
       "<div style=\"display:flex;gap:16px;margin-bottom:20px;\">\n",
       "<div style=\"flex:1;min-width:0;\"><h4 style=\"margin:0 0 8px 0;font-size:14px;color:#000;\">Reference SQL</h4><pre style=\"background:#fff;padding:12px;border-radius:6px;color:#000;overflow-x:auto;font-size:12px;line-height:1.5;margin:0;border:1px solid #ddd;white-space:pre-wrap;word-break:break-word;\">\n",
       "SELECT pc.brand_name, SUM(io.item_total) AS total_revenue\n",
       "FROM itemized_orders io\n",
       "JOIN product_catalogue pc ON io.product_id = pc.product_id\n",
       "WHERE pc.category = &#x27;Snacks &amp; Munchies&#x27;\n",
       "  AND io.order_status = &#x27;delivered&#x27;\n",
       "GROUP BY pc.brand_name\n",
       "ORDER BY total_revenue DESC\n",
       "LIMIT 3;\n",
       "</pre></div>\n",
       "<div style=\"flex:1;min-width:0;\"><h4 style=\"margin:0 0 8px 0;font-size:14px;color:#000;\">Agent SQL</h4><pre style=\"background:#fff;padding:12px;border-radius:6px;color:#000;overflow-x:auto;font-size:12px;line-height:1.5;margin:0;border:1px solid #ddd;white-space:pre-wrap;word-break:break-word;\">SELECT pc.brand_name, SUM(io.item_total) AS total_revenue FROM product_catalogue AS pc JOIN itemized_orders AS io ON pc.product_id = io.product_id WHERE pc.category = &#x27;Snacks &amp; Munchies&#x27; GROUP BY pc.brand_name ORDER BY total_revenue DESC LIMIT 3;</pre></div>\n",
       "</div>\n",
       "<div style=\"display:flex;gap:16px;\">\n",
       "<div style=\"flex:1;min-width:0;overflow-x:auto;\"><h4 style=\"margin:0 0 8px 0;font-size:14px;color:#000;\">Expected Output</h4><table class=\"dataframe compare-table\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>brand_name</th>\n",
       "      <th>total_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Cadbury</td>\n",
       "      <td>12344944.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ferrero</td>\n",
       "      <td>10289819.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ITC</td>\n",
       "      <td>8770143.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>\n",
       "<div style=\"flex:1;min-width:0;overflow-x:auto;\"><h4 style=\"margin:0 0 8px 0;font-size:14px;color:#000;\">Agent Output</h4><table class=\"dataframe compare-table\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>brand_name</th>\n",
       "      <th>total_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Cadbury</td>\n",
       "      <td>12591769.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ferrero</td>\n",
       "      <td>10465097.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ITC</td>\n",
       "      <td>8943593.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>\n",
       "</div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the agent on your new example and display comparison\n",
    "r = run_single_eval(agent_v1, new_example, db_conn)\n",
    "\n",
    "display_comparison(\n",
    "    question=new_example[\"question\"],\n",
    "    reference_sql=new_example[\"reference_sql\"],\n",
    "    agent_sql=r[\"agent_sql\"],\n",
    "    expected_csv=new_example[\"expected_answer\"],\n",
    "    agent_csv=r.get(\"agent_csv\"),\n",
    "    answer_quality_score=r[\"answer_quality_score\"],\n",
    "    answer_quality_reasoning=r[\"answer_quality_reasoning\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307ad22",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "I need help debugging a DSPy agent that's failing evals. I'm providing:\n",
    "1. **Eval Results** — output/scores from my eval run\n",
    "2. **Eval Definition** — metrics, dataset, assertions\n",
    "3. **Agent Definition** — module/signature/program\n",
    "\n",
    "## What to do:\n",
    "\n",
    "**Understand:** Understand what the agent does and what the eval measures.\n",
    "\n",
    "**Analyze Failures:** Identify failing cases, categorize failure modes (wrong format, hallucination, bad reasoning, tool misuse, etc.), and find patterns.\n",
    "\n",
    "**Trace Root Causes:** For each failure pattern, trace to root cause — underspecified signature? wrong module type? bad tool descriptions? buggy eval metric?\n",
    "\n",
    "**Suggest Fixes (Ranked):** Propose concrete fixes ranked by likely impact — exact prompt/signature changes, architecture changes, tool description improvements, eval fixes.\n",
    "\n",
    "## Output constraints:\n",
    "- No filler or preamble. Be direct.\n",
    "- Use tables for failure analysis (case → failure mode → root cause → fix).\n",
    "- Keep each section to 2 lines max.\n",
    "- Show exact code diffs for proposed fixes, not descriptions of what to change.\n",
    "- Total response should fit in 10-15 lines.\n",
    "\n",
    "---\n",
    "\n",
    "### Eval Results:\n",
    "<paste here>\n",
    "\n",
    "### Eval Definition:\n",
    "<paste here>\n",
    "\n",
    "### Agent Definition:\n",
    "<paste here>\n",
    "\"\"\"\n",
    "\n",
    "User query, agent, tracjectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981ba48",
   "metadata": {},
   "source": [
    "## Improving the agent \n",
    "\n",
    "**Why we need it?**\n",
    "\n",
    "Agent V1 has no idea what's in the database. Every time you ask it a question, it has to spend 2-3 calls just running SHOW TABLES and DESCRIBE to figure out what tables exist and what columns they have — before it even starts writing the actual query. That causes two problems:\n",
    "- It's slow and expensive — half the tool calls are just schema discovery, not actual analysis. That's why V1's efficiency avg is 0.84.\n",
    "- It still gets things wrong — reading a DESCRIBE output doesn't tell the agent that itemized_orders has one row per line item not per order, or that SLA breach means actual_delivery_secs > committed_delivery_secs. Without that context, it guesses wrong on the hard queries.\n",
    "\n",
    "Agent V2 fixes this by passing the full schema + documentation upfront as a db_context input field. The agent skips discovery entirely and goes straight to writing the analytical query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "j9e5somzvj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the database context to the agent's instructions to further improve its performance and reduce iterations\n",
    "DB_CONTEXT = \"\"\"\n",
    "## Database: qc_pune.duckdb (DuckDB)\n",
    "Quick-commerce order data for Pune, India (~Nov 2025 – Feb 2026).\n",
    "\n",
    "### Tables\n",
    "\n",
    "1. consumers — Customer profiles with delivery addresses\n",
    "   - consumer_id  (VARCHAR, PK) – unique customer ID, e.g. \"CON-070619\"\n",
    "   - address_id   (VARCHAR, PK) – unique address per customer, e.g. \"CON-070619-A2\"\n",
    "   - gender       (VARCHAR)     – customer gender\n",
    "   - age          (INTEGER)     – customer age\n",
    "   - latitude     (DOUBLE)      – delivery address latitude\n",
    "   - longitude    (DOUBLE)      – delivery address longitude\n",
    "   - address      (VARCHAR)     – full delivery address text\n",
    "   - pincode      (VARCHAR)     – postal pin code\n",
    "   - nearest_store_id (VARCHAR) – ID of the closest store, e.g. \"STR-12\"\n",
    "\n",
    "2. itemized_orders — One row per line item in every order\n",
    "   - order_id      (VARCHAR, PK)   – order identifier, e.g. \"ORD-20251101-000001\"\n",
    "   - order_line_id (INTEGER, PK)   – line number within the order (1, 2, …)\n",
    "   - consumer_id   (VARCHAR)       – FK → consumers.consumer_id\n",
    "   - address_id    (VARCHAR)       – FK → consumers.address_id\n",
    "   - store_id      (VARCHAR)       – FK → stores.store_id\n",
    "   - product_id    (VARCHAR)       – FK → product_catalogue.product_id\n",
    "   - quantity      (INTEGER)       – units ordered\n",
    "   - mrp           (DOUBLE)        – maximum retail price per unit\n",
    "   - discount      (DOUBLE)        – fractional discount (0.08 = 8 %)\n",
    "   - item_total    (DOUBLE)        – line total after discount\n",
    "   - order_timestamp (TIMESTAMP)   – when the order was placed\n",
    "   - order_status  (VARCHAR)       – e.g. \"delivered\", \"cancelled\"\n",
    "   - cancel_reason (VARCHAR, NULL) – reason if cancelled, else NULL\n",
    "   - committed_delivery_secs (INTEGER) – promised delivery time in seconds\n",
    "   - actual_delivery_secs   (INTEGER, NULL) – actual delivery time in seconds\n",
    "   - distance_km   (DOUBLE)       – distance from store to customer (km)\n",
    "\n",
    "3. product_catalogue — Master product list\n",
    "   - product_id        (VARCHAR, PK) – e.g. \"PRD-DB-MLK-10199\"\n",
    "   - product_name      (VARCHAR)     – e.g. \"Britannia Milk Bread\"\n",
    "   - brand_name        (VARCHAR)     – e.g. \"Britannia\", \"Dabur\"\n",
    "   - brand_id          (VARCHAR)     – e.g. \"BRD-79948b7a\"\n",
    "   - category          (VARCHAR)     – top-level category, e.g. \"Dairy & Breakfast\", \"Beverages\"\n",
    "   - sub_category      (VARCHAR)     – e.g. \"Baby Food\", \"Milk\", \"Juices\"\n",
    "   - size              (VARCHAR)     – pack size, e.g. \"250g\", \"1kg\", \"500ml\"\n",
    "   - mrp               (DOUBLE)      – listed MRP\n",
    "   - category_code     (VARCHAR)     – short code, e.g. \"BAB\", \"DB\", \"BV\"\n",
    "   - sub_category_code (VARCHAR)     – short code, e.g. \"BF\", \"MLK\", \"JU\"\n",
    "\n",
    "4. stores — Store locations and service areas\n",
    "   - store_id         (VARCHAR, PK) – e.g. \"STR-12\"\n",
    "   - store_name       (VARCHAR)     – human-readable store name\n",
    "   - latitude         (DOUBLE)      – store latitude\n",
    "   - longitude        (DOUBLE)      – store longitude\n",
    "   - radius_km        (DOUBLE)      – delivery radius in km\n",
    "   - pincode          (VARCHAR)     – store pin code\n",
    "   - area             (VARCHAR)     – area / neighbourhood name\n",
    "   - serviceable_area (VARCHAR)     – broader serviceable zone\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AnalystSignature(dspy.Signature):\n",
    "    \"\"\"You are a data analyst working with a DuckDB database.\n",
    "\n",
    "    Given a user's question, write and execute SQL queries and provide a\n",
    "    clear, accurate answer grounded in the data.\n",
    "\n",
    "    Do NOT waste iterations on SHOW TABLES or DESCRIBE — go straight to analytical queries.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"A natural language question about the data\")\n",
    "    db_context: str = dspy.InputField(desc=\"Database schema and documentation\")\n",
    "    answer: str = dspy.OutputField(desc=\"A concise, data-backed answer to the question\")\n",
    "\n",
    "\n",
    "agent_v2 = dspy.ReAct(AnalystSignature, tools=[execute_sql], max_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37244833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 product categories ordered on weekends are:\n",
      "1. Personal Care (189634 orders)\n",
      "2. Beverages (188363 orders)\n",
      "3. Instant & Ready to Eat (188342 orders)\n",
      "\n",
      "Total iterations (with DB context): 2\n"
     ]
    }
   ],
   "source": [
    "# Run the same question with the new agent that has DB context in its instructions\n",
    "weekend_orders = agent_v2(\n",
    "    question=\"Which top 3 product categories that are often ordered on weekends?\",\n",
    "    db_context=DB_CONTEXT,\n",
    ")\n",
    "print(weekend_orders.answer)\n",
    "\n",
    "trajectory = weekend_orders.trajectory\n",
    "observations = [k for k in trajectory if k.startswith(\"observation_\")]\n",
    "print(f\"\\nTotal iterations (with DB context): {len(observations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73ec8bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eval on 16 examples with agent_v2\n",
      "\n",
      "#   ID                                  Difficulty   Efficiency   SQLValid   Recovery   Answer Quality\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1   1_product_count                     easy         1.00         1.00       1.00       1.00       (6.4s)\n",
      "2   2_unique_categories                 easy         1.00         1.00       1.00       1.00       (8.8s)\n",
      "3   3_store_count                       easy         1.00         1.00       1.00       1.00       (4.1s)\n",
      "4   4_avg_age_by_gender                 easy         1.00         1.00       1.00       0.80       (5.3s)\n",
      "5   5_orders_by_status                  easy         1.00         1.00       1.00       1.00       (11.5s)\n",
      "6   6_top_store_delivered               easy         1.00         1.00       1.00       1.00       (17.0s)\n",
      "7   7_store_performance_scorecard       medium       1.00         1.00       1.00       0.70       (37.6s)\n",
      "8   8_demand_hour_dayofweek             medium       1.00         1.00       1.00       0.70       (14.6s)\n",
      "9   9_user_retention                    hard         1.00         0.50       0.75       0.80       (30.9s)\n",
      "10  10_basket_affinity                  hard         1.00         1.00       1.00       0.90       (18.8s)\n",
      "11  11_high_value_churned               hard         1.00         1.00       1.00       0.30       (11.5s)\n",
      "12  12_cancellation_rca                 hard         1.00         1.00       1.00       0.70       (13.6s)\n",
      "13  13_4week_moving_avg                 hard         1.00         1.00       1.00       0.70       (42.7s)\n",
      "14  14_delivery_distance_sla_breach     hard         1.00         1.00       1.00       0.70       (20.1s)\n",
      "15  15_impossible_salary                impossible   1.00         1.00       1.00       1.00       (6.6s)\n",
      "16  16_impossible_weather               impossible   1.00         1.00       1.00       1.00       (3.5s)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "AVG                                                  1.00         0.97       0.98       0.83       (253s)\n"
     ]
    }
   ],
   "source": [
    "agent_v2 = dspy.ReAct(AnalystSignature, tools=[execute_sql], max_iters=10)\n",
    "eval_results = run_eval(\n",
    "    agent_v2, DATASET, db_conn, agent_name=\"agent_v2\", db_context=DB_CONTEXT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rjpyzl377t",
   "metadata": {},
   "source": [
    "## Optimizing the Agent\n",
    "\n",
    "DSPy optimizers automatically improve your AI program's performance by tuning prompts and model weights — so instead of manually tweaking prompt wording through trial and error, you let an algorithm systematically find what works best for your specific task and metric.\n",
    "\n",
    "They take three inputs: \n",
    "- Your DSPy program (simple or complex)\n",
    "- A scoring metric that defines what \"good\" looks like\n",
    "- Set of training examples (as few as 5–10, labels optional)\n",
    "\n",
    "From there, the optimizer iterates and refines your program to maximize that metric — achieving results that are often better than hand-crafted prompts and far more reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vs6mhhpjgvb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_metric(example, pred, trace=None) -> float:\n",
    "    \"\"\"Score agent output using the combined LLM judge.\"\"\"\n",
    "    agent_sql = get_last_analytical_sql(pred.trajectory)\n",
    "    if not agent_sql:\n",
    "        return 0.0\n",
    "    result = answer_quality(\n",
    "        agent_sql,\n",
    "        example.reference_sql,\n",
    "        example.expected_answer,\n",
    "        example.question,\n",
    "        db_conn,\n",
    "    )\n",
    "    return result[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "w65wiqnh6z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 14 examples\n"
     ]
    }
   ],
   "source": [
    "# Build training set from eval dataset (exclude impossible questions)\n",
    "trainset = [\n",
    "    dspy.Example(\n",
    "        question=ex[\"question\"],\n",
    "        db_context=DB_CONTEXT,\n",
    "        expected_answer=ex[\"expected_answer\"],\n",
    "        reference_sql=ex[\"reference_sql\"],\n",
    "    ).with_inputs(\"question\", \"db_context\")\n",
    "    for ex in DATASET\n",
    "    if not ex[\"difficulty\"] == \"impossible\"\n",
    "]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0xszo34cq1ka",
   "metadata": {},
   "source": [
    "### GEPA (Genetic-Pareto Reflective Prompt Evolution) Optimizer\n",
    "\n",
    "- GEPA (Genetic-Pareto) is a reflective prompt optimizer that improves your AI program by actually understanding what went wrong — not just trying random variations. Traditional optimizers and RL methods rely on scalar reward signals (just a number saying \"good\" or \"bad\"), which means they need thousands of rollouts to stumble toward better prompts. \n",
    "- GEPA instead reads the full execution traces — reasoning steps, tool outputs, error messages — and uses an LLM to reflect on why something failed and propose targeted fixes. Think of it as the difference between a student who only sees their test score versus one who gets detailed feedback on each answer.\n",
    "\n",
    "Why it matters: \n",
    "- GEPA outperforms GRPO (a leading RL method) by 6% on average and up to 20%, while using up to 35x fewer rollouts. It also beats MIPROv2 by over 10%. [arXiv](https://arxiv.org/abs/2507.19457) \n",
    "- It achieves this through evolutionary search combined with Pareto-aware selection — keeping the best-performing prompt variants across multiple objectives simultaneously, rather than optimizing for a single metric.\n",
    "\n",
    "A key advantage is that GEPA supports domain-specific textual feedback. Your metric function can return not just a score but a natural language explanation of what needs improvement, which GEPA uses directly to guide its next round of prompt evolution. This makes it especially powerful for complex, multi-module programs where you want fine-grained control over how each component improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "i7k0fjr4qz",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 21:41:39 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 42 metric calls of the program. This amounts to 3.00 full evals on the train set.\n",
      "2026/02/26 21:41:39 WARNING dspy.teleprompt.gepa.gepa: No valset provided; Using trainset as valset. This is useful as an inference-time scaling strategy where you want GEPA to find the best solutions for the provided tasks in the trainset, as it makes GEPA overfit prompts to the provided trainset. In order to ensure generalization and perform well on unseen tasks, please provide separate trainset and valset. Provide the smallest valset that is just large enough to match the downstream task distribution, while keeping trainset as large as possible.\n",
      "2026/02/26 21:41:39 INFO dspy.teleprompt.gepa.gepa: Using 14 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget. GEPA requires you to provide the smallest valset that is just large enough to match your downstream task distribution, while providing as large trainset as possible.\n",
      "2026/02/26 21:43:20 INFO dspy.evaluate.evaluate: Average Metric: 8.8 / 14 (62.9%)\n",
      "2026/02/26 21:43:20 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.6285714285714287 over 14 / 14 examples\n",
      "2026/02/26 21:43:20 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.6285714285714287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.50 / 2 (75.0%): 100%|██████████| 2/2 [02:22<00:00, 71.39s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 21:45:42 INFO dspy.evaluate.evaluate: Average Metric: 1.5 / 2 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 21:46:01 WARNING dspy.teleprompt.gepa.gepa_utils: The score returned by the metric with pred_name is different from the overall metric score. This can indicate 2 things: Either the metric is non-deterministic (e.g., LLM-as-judge, Semantic score, etc.) or the metric returned a score specific to pred_name that differs from the module level score. Currently, GEPA does not support predictor level scoring (support coming soon), and only requires a feedback text to be provided, which can be specific to the predictor or program level. GEPA will ignore the differing score returned, and instead use module level score. You can safely ignore this warning if using a semantic metric, however, if this mismatch is caused due to predictor scoring, please return module-level scores. To disable this warning, set warn_on_score_mismatch=False.\n",
      "2026/02/26 21:46:08 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for react: You are a data analyst working with a DuckDB database.\n",
      "\n",
      "Your primary goal is to analyze quick-commerce order data for Pune, India (approximately November 2025 – February 2026).\n",
      "\n",
      "Given a user's question, your task is to write and execute SQL queries and then provide a clear, accurate answer grounded in the data.\n",
      "\n",
      "You are equipped with schema information for the tables, so you **DO NOT** need to use `SHOW TABLES` or `DESCRIBE` — go straight to analytical queries.\n",
      "\n",
      "You are an Agent. In each episode, you will be given the fields `question`, `db_context` as input. You can also see your past trajectory.\n",
      "Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n",
      "\n",
      "To do this, you will interleave `next_thought`, `next_tool_name`, and `next_tool_args` in each turn, and also when finishing the task.\n",
      "After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
      "\n",
      "When writing `next_thought`, you should reason about the current situation and plan for future steps.\n",
      "When selecting the `next_tool_name` and its `next_tool_args`, the tool must be one of:\n",
      "\n",
      "(1) `execute_sql`, whose description is \"Execute a SQL query against the DuckDB database. Write valid DuckDB SQL. Use LIMIT on exploratory queries to prevent large outputs, but for final analytical queries, ensure all necessary data is retrieved.\" It takes arguments `sql` (string) and `limit` (integer, default 50).\n",
      "(2) `finish`, whose description is \"Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.\" It takes no arguments other than the `answer`.\n",
      "\n",
      "When completing the task using `finish`, ensure the `answer` argument provides a concise and direct response to the user's question, summarizing the data analysis performed. If the result is a list of items, present them clearly; if it's a table-like output, mention the relevant columns and how many entries were found.\n",
      "\n",
      "**Key Data Analysis Considerations:**\n",
      "\n",
      "*   **Date Ranges:** When working with dates, specify the `order_timestamp` column. Remember that the data spans from approximately November 2025 to February 2026. Be precise with date ranges using `order_timestamp >= 'YYYY-MM-DD HH:MM:SS'` and `order_timestamp < 'YYYY-MM-DD HH:MM:SS'`. For example, Nov-Dec 2025 means `order_timestamp >= '2025-11-01 00:00:00' AND order_timestamp < '2026-01-01 00:00:00'`. Feb 2026 means `order_timestamp >= '2026-02-01 00:00:00' AND order_timestamp < '2026-03-01 00:00:00'`.\n",
      "*   **Order Status:** Unless specified otherwise, most analytical queries concerning spending or successful orders should filter for `order_status = 'delivered'` to exclude cancelled or pending orders.\n",
      "*   **Monetary Values:** Monetary values are in Indian Rupees (₹).\n",
      "*   **Identifying Churn Risks:** To identify high-value churn risks (consumers who spent significantly but stopped ordering), a common strategy involves:\n",
      "    1.  Calculating total spending for a specific past period.\n",
      "    2.  Filtering consumers based on a spending threshold.\n",
      "    3.  Identifying consumers who have placed orders in a more recent period.\n",
      "    4.  Using a `LEFT JOIN` and `WHERE ... IS NULL` clause to find consumers from step 2 who are *not* in the group from step 3. The SQL needs to be structured to get the final result using one query if possible.\n",
      "\n",
      "**Output Format for `finish`:**\n",
      "\n",
      "The `answer` in the `finish` tool call should be a string that directly addresses the user's question.\n",
      "If the query results in multiple rows or a list, summarize them clearly. For instance, \"The unique product categories are: A, B, C, D.\" or \"The consumers identified as high-value churn risks are: CON-XXXXXX, CON-YYYYYY, CON-ZZZZZZ. A total of N consumers were found.\"\n",
      "When providing a list of IDs, if the list is very long, state the count and provide the first few entries as an example, or mention that a complete list is available upon request (if applicable).\n",
      "2026/02/26 21:48:23 INFO dspy.evaluate.evaluate: Average Metric: 1.6 / 2 (80.0%)\n",
      "2026/02/26 21:48:23 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New subsample score 1.6 is better than old score 1.5. Continue to full eval and add to candidate pool.\n",
      "2026/02/26 21:50:10 INFO dspy.evaluate.evaluate: Average Metric: 10.75 / 14 (76.8%)\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Found a better program on the valset with score 0.7678571428571429.\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Valset score for new program: 0.7678571428571429 (coverage 14 / 14)\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Val aggregate for new program: 0.7678571428571429\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.95, 4: 1.0, 5: 1.0, 6: 0.8, 7: 0.7, 8: 0.3, 9: 0.7, 10: 0.5, 11: 0.7, 12: 0.8, 13: 0.3}\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 0.8, 7: 0.7, 8: 0.3, 9: 0.9, 10: 0.5, 11: 0.7, 12: 0.8, 13: 0.3}\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Valset pareto front aggregate score: 0.7857142857142857\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: {0: {0, 1}, 1: {0, 1}, 2: {0, 1}, 3: {0}, 4: {0, 1}, 5: {0, 1}, 6: {1}, 7: {1}, 8: {1}, 9: {0}, 10: {1}, 11: {1}, 12: {1}, 13: {1}}\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.7678571428571429\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 1\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.7678571428571429\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 1\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n",
      "2026/02/26 21:50:10 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 1 score: 0.7678571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.80 / 2 (40.0%): 100%|██████████| 2/2 [00:48<00:00, 24.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 21:50:58 INFO dspy.evaluate.evaluate: Average Metric: 0.8 / 2 (40.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 21:51:06 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for extract.predict: You are a data analyst working with a DuckDB database.\n",
      "\n",
      "Given a user's question, write and execute SQL queries and provide a\n",
      "clear, accurate answer grounded in the data.\n",
      "\n",
      "Do NOT waste iterations on SHOW TABLES or DESCRIBE — go straight to analytical queries.\n",
      "\n",
      "When providing your final answer, do not include an `answer` key or similar. Respond directly with the answer text itself. Any output that is not recognized as a tool call will be considered the final answer.\n",
      "2026/02/26 21:51:50 INFO dspy.evaluate.evaluate: Average Metric: 1.9 / 2 (95.0%)\n",
      "2026/02/26 21:51:50 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score 1.9 is better than old score 0.8. Continue to full eval and add to candidate pool.\n",
      "2026/02/26 21:53:33 INFO dspy.evaluate.evaluate: Average Metric: 9.7 / 14 (69.3%)\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Valset score for new program: 0.6928571428571428 (coverage 14 / 14)\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Val aggregate for new program: 0.6928571428571428\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Individual valset scores for new program: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 0.0, 6: 1.0, 7: 0.6, 8: 1.0, 9: 0.8, 10: 0.2, 11: 0.4, 12: 0.6, 13: 0.1}\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New valset pareto front scores: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 0.7, 8: 1.0, 9: 0.9, 10: 0.5, 11: 0.7, 12: 0.8, 13: 0.3}\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Valset pareto front aggregate score: 0.85\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Updated valset pareto front programs: {0: {0, 1, 2}, 1: {0, 1, 2}, 2: {0, 1, 2}, 3: {0, 2}, 4: {0, 1, 2}, 5: {0, 1}, 6: {2}, 7: {1}, 8: {2}, 9: {0}, 10: {1}, 11: {1}, 12: {1}, 13: {1}}\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best valset aggregate score so far: 0.7678571428571429\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on valset: 1\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on valset: 0.7678571428571429\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Linear pareto front program index: 1\n",
      "2026/02/26 21:53:33 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New program candidate index: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA optimization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/26 21:56:01 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=100000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently None)  if the reason for truncation is repetition.\n"
     ]
    }
   ],
   "source": [
    "# GEPA requires a 5-argument metric: (gold, pred, trace, pred_name, pred_trace)\n",
    "def gepa_metric(example, pred, trace=None, pred_name=None, pred_trace=None) -> float:\n",
    "    \"\"\"Score agent output using the combined LLM judge (GEPA-compatible signature).\"\"\"\n",
    "    agent_sql = get_last_analytical_sql(pred.trajectory)\n",
    "    if not agent_sql:\n",
    "        return 0.0\n",
    "    result = answer_quality(\n",
    "        agent_sql,\n",
    "        example.reference_sql,\n",
    "        example.expected_answer,\n",
    "        example.question,\n",
    "        db_conn,\n",
    "    )\n",
    "    return result[\"score\"]\n",
    "\n",
    "\n",
    "# Optimize agent_v2 with GEPA\n",
    "gepa_optimizer = dspy.GEPA(\n",
    "    metric=gepa_metric,\n",
    "    max_full_evals=3,\n",
    "    reflection_lm=lm,  # LM used for reflective reasoning\n",
    "    num_threads=4,  # parallelize metric evaluations\n",
    "    use_merge=False,  # skip merge phase\n",
    "    reflection_minibatch_size=2,  # fewer examples per reflection step\n",
    ")\n",
    "\n",
    "agent_v2_fresh = dspy.ReAct(AnalystSignature, tools=[execute_sql], max_iters=10)\n",
    "gepa_agent = gepa_optimizer.compile(agent_v2_fresh, trainset=trainset)\n",
    "print(\"GEPA optimization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cc116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized agent\n",
    "gepa_agent.save(\"gepa_agent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8de248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy LM via OpenRouter\n",
    "lm = dspy.LM(\n",
    "    \"openrouter/google/gemini-2.5-flash-lite\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    max_tokens=100000,\n",
    "    cache=False,\n",
    ")\n",
    "dspy.configure(lm=lm, adapter=dspy.ChatAdapter(), track_usage=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb21a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eval on 16 examples with agent_v2\n",
      "\n",
      "#   ID                                  Difficulty   Efficiency   SQLValid   Recovery   Answer Quality\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1   1_product_count                     easy         1.00         1.00       1.00       1.00       (6.8s)\n",
      "2   2_unique_categories                 easy         1.00         1.00       1.00       1.00       (4.0s)\n",
      "3   3_store_count                       easy         1.00         1.00       1.00       0.90       (4.1s)\n",
      "4   4_avg_age_by_gender                 easy         1.00         1.00       1.00       0.80       (5.6s)\n",
      "5   5_orders_by_status                  easy         1.00         1.00       1.00       1.00       (5.5s)\n",
      "6   6_top_store_delivered               easy         1.00         1.00       1.00       0.00       (7.8s)\n",
      "7   7_store_performance_scorecard       medium       1.00         1.00       1.00       0.70       (37.6s)\n",
      "8   8_demand_hour_dayofweek             medium       1.00         1.00       1.00       0.70       (17.5s)\n",
      "9   9_user_retention                    hard         0.25         0.10       0.75       0.50       (170.0s)\n",
      "10  10_basket_affinity                  hard         1.00         0.50       0.75       0.80       (13.8s)\n",
      "11  11_high_value_churned               hard         1.00         1.00       1.00       0.20       (9.1s)\n",
      "12  12_cancellation_rca                 hard         1.00         1.00       1.00       0.40       (9.1s)\n",
      "13  13_4week_moving_avg                 hard         1.00         1.00       1.00       0.30       (16.0s)\n",
      "14  14_delivery_distance_sla_breach     hard         1.00         1.00       1.00       0.20       (11.9s)\n",
      "15  15_impossible_salary                impossible   1.00         1.00       1.00       1.00       (4.0s)\n",
      "16  16_impossible_weather               impossible   1.00         1.00       1.00       1.00       (4.1s)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "AVG                                                  0.95         0.91       0.97       0.66       (327s)\n"
     ]
    }
   ],
   "source": [
    "eval_results = run_eval(\n",
    "    agent_v2, DATASET, db_conn, agent_name=\"agent_v2\", db_context=DB_CONTEXT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it back later\n",
    "pregenerated_gepa_agent = dspy.ReAct(\n",
    "    AnalystSignature, tools=[execute_sql], max_iters=10\n",
    ")\n",
    "pregenerated_gepa_agent.load(\"pregenerated_gepa_agent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c58b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eval on 16 examples with gepa_agent\n",
      "\n",
      "#   ID                                  Difficulty   Efficiency   SQLValid   Recovery   Answer Quality\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1   1_product_count                     easy         1.00         1.00       1.00       1.00       (11.0s)\n",
      "2   2_unique_categories                 easy         1.00         1.00       1.00       1.00       (8.0s)\n",
      "3   3_store_count                       easy         1.00         1.00       1.00       1.00       (3.6s)\n",
      "4   4_avg_age_by_gender                 easy         1.00         1.00       1.00       0.80       (4.7s)\n",
      "5   5_orders_by_status                  easy         1.00         1.00       1.00       1.00       (21.1s)\n",
      "6   6_top_store_delivered               easy         1.00         1.00       1.00       1.00       (8.6s)\n",
      "7   7_store_performance_scorecard       medium       1.00         1.00       1.00       0.70       (68.5s)\n",
      "8   8_demand_hour_dayofweek             medium       1.00         1.00       1.00       0.70       (14.5s)\n",
      "9   9_user_retention                    hard         1.00         0.50       0.75       0.70       (38.1s)\n",
      "10  10_basket_affinity                  hard         1.00         1.00       1.00       0.90       (18.0s)\n",
      "11  11_high_value_churned               hard         1.00         1.00       1.00       0.40       (16.1s)\n",
      "12  12_cancellation_rca                 hard         1.00         1.00       1.00       0.90       (11.1s)\n",
      "13  13_4week_moving_avg                 hard         1.00         1.00       1.00       0.70       (19.8s)\n",
      "14  14_delivery_distance_sla_breach     hard         1.00         1.00       1.00       0.40       (10.7s)\n",
      "15  15_impossible_salary                impossible   1.00         1.00       1.00       1.00       (13.1s)\n",
      "16  16_impossible_weather               impossible   1.00         1.00       1.00       1.00       (4.7s)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "AVG                                                  1.00         0.97       0.98       0.82       (272s)\n"
     ]
    }
   ],
   "source": [
    "eval_results = run_eval(\n",
    "    pregenerated_gepa_agent,\n",
    "    DATASET,\n",
    "    db_conn,\n",
    "    agent_name=\"pregenerated_gepa_agent\",\n",
    "    db_context=DB_CONTEXT,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-agents-workshop (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
